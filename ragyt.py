# -*- coding: utf-8 -*-
"""RAGYT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ANqBMWQ5Gn1DXsqs7BEQ47eEACIFpju
"""

!pip install requests beautifulsoup4 langchain openai chromadb langchain-openai langchain-community huggingface_hub

import os
import time
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFaceHub
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# --- Set up Hugging Face API token ---
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR_HUGGING_FACE_TOKEN_HERE"  # Replace with your own token

# --- Load documents from website ---
loader = WebBaseLoader(web_paths=["https://www.educosys.com/course/genai"])
docs = loader.load()

# --- Split the documents into chunks ---
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

print(f"Total Chunks (splits) created: {len(splits)}")
print(f"Preview of first chunk:\n{splits[0].page_content[:300]}...\n")

# --- Create vector store ---
embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function)
retriever = vectorstore.as_retriever()

# --- Set up the LLM using Hugging Face's 't5-small' model ---
llm = HuggingFaceHub(
    repo_id="t5-small",
    model_kwargs={"temperature": 0.5, "max_new_tokens": 512}
)

# --- Define the RAG chain ---
prompt = hub.pull("rlm/rag-prompt")

# âœ… Fixed format_docs function
def format_docs(docs):
    return "\n".join(getattr(doc, "page_content", doc) for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# --- Ask a question ---
question = "Are the recordings of the course available? For how long?"
response = rag_chain.invoke(question)

print("ðŸ“Œ Question:", question)
print("ðŸ§  Answer:", response)